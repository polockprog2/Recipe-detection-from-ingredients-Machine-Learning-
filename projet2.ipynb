{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "%matplotlib inline\n",
    "# neccasary packages are loaded\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "from IPython.display import display, Markdown\n",
    "from ast import literal_eval\n",
    "import calendar\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "# Load Excel file\n",
    "file_path = 'full_dataset.csv' \n",
    "recipes = pd.read_csv(file_path)\n",
    "print(recipes.head())\n",
    "recipes.info()\n",
    "recipes.head()\n",
    "recipes.rename(columns={'Unnamed: 0': 'ID','directions': 'procedure', 'title': 'name'},inplace=True, errors='raise')\n",
    "recipes.head()\n",
    "recipes = recipes.astype({'ID': int,\n",
    "                'name': str,\n",
    "                'ingredients': 'object',\n",
    "                'procedure': 'object',\n",
    "                'link': str,\n",
    "                'source': 'category',\n",
    "                'NER': 'object'})\n",
    "recipes['n_NER'] = recipes['NER'].apply(len)\n",
    "recipes['n_procedures'] = recipes['procedure'].apply(len)\n",
    "recipes.head()\n",
    "recipes_old = recipes[:]\n",
    "recipes = recipes[recipes['n_NER'] > 0]\n",
    "recipes = recipes[recipes['n_procedures'] > 0]\n",
    "recipes['source'].unique()\n",
    "unique_names = len(recipes['name'].unique())\n",
    "number_entries = len(recipes)\n",
    "number_entries_old = len(recipes_old)\n",
    "display(Markdown(f'Number of all entries: {number_entries:.0f} vs Number of all entries before cleaning: {number_entries_old :.0f}'))\n",
    "display(Markdown(f'Number of unique names: {unique_names :.0f}'))\n",
    "average_n_ingridients = recipes.groupby('source')['n_NER'].mean()\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.set_title('Distribution of number ingridients by sources')\n",
    "average_n_ingridients.plot(kind='barh', stacked=False, ax=ax)\n",
    "plt.show()\n",
    "average_n_procedures = recipes.groupby('source')['n_procedures'].mean()\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.set_title('Distribution of number procedure steps by sources')\n",
    "average_n_procedures.plot(kind='barh', stacked=False, ax=ax)\n",
    "plt.show()\n",
    "NER_exploded = recipes.explode('NER')\n",
    "ingridients = NER_exploded['NER']\n",
    "unique_NER_values = NER_exploded['NER'].unique()\n",
    "display(Markdown(f'Number of unique ingridients: {len(unique_NER_values) :.0f}'))\n",
    "print(unique_NER_values[:200])\n",
    "wordcloud = WordCloud(width=800, height=800, background_color='white').generate(' '.join(ingridients[:1000]))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy: 0.7563963794374637\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Gathered       0.77      0.95      0.85    328949\n",
      "   Recipes1M       0.60      0.22      0.32    117280\n",
      "\n",
      "    accuracy                           0.76    446229\n",
      "   macro avg       0.69      0.58      0.59    446229\n",
      "weighted avg       0.73      0.76      0.71    446229\n",
      "\n",
      "Naive Bayes:\n",
      "Accuracy: 0.7471858619677341\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Gathered       0.77      0.93      0.84    328949\n",
      "   Recipes1M       0.55      0.22      0.32    117280\n",
      "\n",
      "    accuracy                           0.75    446229\n",
      "   macro avg       0.66      0.58      0.58    446229\n",
      "weighted avg       0.71      0.75      0.71    446229\n",
      "\n",
      "Gradient Boosting:\n",
      "Accuracy: 0.7508073209047372\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Gathered       0.75      0.99      0.85    328949\n",
      "   Recipes1M       0.70      0.09      0.16    117280\n",
      "\n",
      "    accuracy                           0.75    446229\n",
      "   macro avg       0.73      0.54      0.51    446229\n",
      "weighted avg       0.74      0.75      0.67    446229\n",
      "\n",
      "Support Vector Machine:\n",
      "Accuracy: 0.7568759538263986\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Gathered       0.77      0.95      0.85    328949\n",
      "   Recipes1M       0.60      0.22      0.32    117280\n",
      "\n",
      "    accuracy                           0.76    446229\n",
      "   macro avg       0.69      0.58      0.59    446229\n",
      "weighted avg       0.73      0.76      0.71    446229\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.7520622819225107\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Gathered       0.76      0.97      0.85    328949\n",
      "   Recipes1M       0.63      0.14      0.23    117280\n",
      "\n",
      "    accuracy                           0.75    446229\n",
      "   macro avg       0.69      0.56      0.54    446229\n",
      "weighted avg       0.72      0.75      0.69    446229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "\n",
    "\n",
    "valid_recipes = recipes[(recipes['ingredients'].notnull())]\n",
    "valid_recipes['text'] = valid_recipes['ingredients'] \n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X = tfidf.fit_transform(valid_recipes['text'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "valid_recipes['source_encoded'] = label_encoder.fit_transform(valid_recipes['source'])\n",
    "y = valid_recipes['source_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logistic))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logistic, target_names=label_encoder.classes_))\n",
    "\n",
    "# Naive Bayes Classifier (MultinomialNB for text data)\n",
    "naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(X_train, y_train)\n",
    "y_pred_naive_bayes = naive_bayes_model.predict(X_test)\n",
    "print(\"Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_naive_bayes))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_naive_bayes, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gradient_boosting_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, random_state=42)\n",
    "gradient_boosting_model.fit(X_train, y_train)\n",
    "y_pred_gradient_boosting = gradient_boosting_model.predict(X_test)\n",
    "print(\"Gradient Boosting:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gradient_boosting))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_gradient_boosting, target_names=label_encoder.classes_))\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_model = LinearSVC(max_iter=500,dual='auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(\"Support Vector Machine:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_))\n",
    "\n",
    "# Random Forest\n",
    "random_forest_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "y_pred_random_forest = random_forest_model.predict(X_test)\n",
    "print(\"Random Forest:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_random_forest))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_random_forest, target_names=label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
